Sure, let's talk about why mathematicians say some things are undefined. In the last video, we talked about why any non-zero number divided by zero is left as undefined. But this might make you wonder about another situation: what happens when we have zero divided by zero?

Let's think about $$ \frac{0}{0} $$. We can look at this in a couple of ways. 

First, let's get closer and closer to zero by dividing numbers by themselves. For example:
- $$ \frac{0.1}{0.1} = 1 $$
- $$ \frac{0.001}{0.001} = 1 $$
- $$ \frac{0.000001}{0.000001} = 1 $$

No matter how close we get to zero, the result is always 1. This is true even if we use negative numbers:
- $$ \frac{-0.1}{-0.1} = 1 $$

You might think, based on this, that zero divided by zero should be 1.

But there's another way to look at it. What if we divide zero by numbers getting closer and closer to zero? Like this:
- $$ \frac{0}{0.1} = 0 $$
- $$ \frac{0}{0.001} = 0 $$
- $$ \frac{0}{0.000001} = 0 $$

Here, the result is always zero, even with negative numbers:
- $$ \frac{0}{-0.1} = 0 $$

This time, you might think zero divided by zero should be zero.

Because both ways of thinking are equally valid, mathematicians can't decide between them. So, to keep things consistent with other parts of math, they say $$ \frac{0}{0} $$ is undefined. That way, we keep our math rules neat and consistent.