Imagine trying to use words  to describe every scene in a film, every note in your favorite song, or every street in your town. Now imagine trying to do it using  only the numbers 1 and 0. Every time you use the Internet to watch a movie, listen to music, or check directions, that’s exactly what your device is doing, using the language of binary code. Computers use binary because  it's a reliable way of storing data. For example, a computer's main memory is made of transistors that switch between either high  or low voltage levels, such as 5 volts and 0 volts. Voltages sometimes oscillate, but since there are only two options, a value of 1 volt  would still be read as "low." That reading is done by  the computer’s processor, which uses the transistors’ states  to control other computer devices according to software instructions. The genius of this system  is that a given binary sequence doesn't have a pre-determined meaning  on its own. Instead, each type of data is encoded in binary according to a separate  set of rules. Let’s take numbers. In normal decimal notation,  each digit is multiplied by 10 raised  to the value of its position, starting from zero on the right. So 84 in decimal form is 4x10⁰ + 8x10¹. Binary number notation works similarly, but with each position  based on 2 raised to some power. So 84 would be written as follows: Meanwhile, letters are interpreted  based on standard rules like UTF-8, which assigns each character to a specific group of 8-digit binary strings. In this case, 01010100 corresponds  to the letter T. So, how can you know whether  a given instance of this sequence is supposed to mean T or 84? Well, you can’t from seeing  the string alone – just as you can’t tell what the sound "da" means from hearing it in isolation. You need context to tell whether you're hearing Russian, Spanish, or English. And you need similar context to tell whether you’re looking  at binary numbers or binary text. Binary code is also used for  far more complex types of data. Each frame of this video, for instance, is made of hundreds  of thousands of pixels. In color images, every pixel is represented  by three binary sequences that correspond to the primary colors. Each sequence encodes a number that determines the intensity of that particular color. Then, a video driver program transmits  this information to the millions of liquid crystals  in your screen to make all the different hues  you see now. The sound in this video  is also stored in binary, with the help of a technique  called pulse code modulation. Continuous sound waves are digitized by taking "snapshots" of their  amplitudes every few milliseconds. These are recorded as numbers  in the form of binary strings, with as many as 44,000 for every second of sound. When they’re read by  your computer’s audio software, the numbers determine how quickly  the coils in your speakers should vibrate to create sounds of different frequencies. All of this requires billions  and billions of bits. But that amount can be reduced  through clever compression formats. For example, if a picture has 30 adjacent  pixels of green space, they can be recorded as "30 green" instead of coding each pixel separately - a process known as run-length encoding. These compressed formats are themselves  written in binary code. So is binary the end-all-be-all  of computing? Not necessarily. There’s been research  into ternary computers, with circuits in three possible states, and even quantum computers, whose circuits can be in multiple states simultaneously. But so far, none of these has provided as much physical stability  for data storage and transmission. So for now, everything you see, hear, and read through your screen comes to you as the result  of a simple "true" or "false" choice, made billions of times over. 